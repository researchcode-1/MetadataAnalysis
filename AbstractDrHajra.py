# -*- coding: utf-8 -*-
"""AbstractWork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hDF1WOpiAcQwWEproqrDkyqrUcKmEeE1
"""

from google.colab import drive
drive.mount('/content/drive')

# AbstractsCleaneddataset
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/Colab_data/AbstractsCleaneddataset.csv')
df= df.iloc[0:200]
df.shape

df.columns

pip install sentence-transformers scikit-learn pandas

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

"""Utilize Sentence-BERT (SBERT) or OpenAI embeddings to compute embeddings for the abstracts.
Cluster the abstracts into semantically similar groups, which may reveal hidden connections between papers or topics.
"""

abstracts = df['Abstract'].dropna().tolist()

# Load a pre-trained SBERT model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose a compact, fast model

# Compute embeddings for all abstracts
embeddings = model.encode(abstracts, show_progress_bar=True)

# Test different cluster numbers and calculate silhouette scores
silhouette_scores = []
cluster_range = range(2, 11)

for n_clusters in cluster_range:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)
    silhouette_scores.append(silhouette_score(embeddings, labels))

# Plot silhouette scores
plt.plot(cluster_range, silhouette_scores, marker='o')
plt.title("Silhouette Score vs. Number of Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()

# Set the optimal number of clusters
optimal_clusters = 3  # Replace this with the best value from the previous step

# Perform KMeans clustering
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
labels = kmeans.fit_predict(embeddings)

# Add the cluster labels to the original dataframe
df['Cluster'] = labels

# Save the results to a CSV file
output_path = 'clustered_abstracts.csv'
df.to_csv(output_path, index=False)
print(f"Clustered data saved to {output_path}")

# Display sample abstracts from each cluster
for cluster in range(optimal_clusters):
    print(f"Cluster {cluster} Examples:")
    print(df[df['Cluster'] == cluster]['Abstract'].sample(5, random_state=42).tolist())
    print("\n")

from sklearn.manifold import TSNE
import numpy as np

# Reduce dimensions to 2D using t-SNE
tsne = TSNE(n_components=2, random_state=42)
reduced_embeddings = tsne.fit_transform(embeddings)

# Plot the clusters
plt.figure(figsize=(10, 8))
for cluster in range(optimal_clusters):
    cluster_points = reduced_embeddings[np.array(labels) == cluster]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f"Cluster {cluster}")

plt.legend()
plt.title("t-SNE Visualization of Clusters")
plt.show()



"""Build a Topic Affiliation Network"""

df.columns

from collections import defaultdict
# Extract authors and topics
authors_by_topic = defaultdict(list)
for _, row in df.iterrows():
    topic = row['Topic name']
    authors = row['Authors'].split(',')  # Assuming authors are comma-separated
    authors_by_topic[topic].extend([author.strip() for author in authors])

pip install --upgrade bertopic

from bertopic import BERTopic

# Use BERTopic to generate topics from abstracts
model = BERTopic()
topics, probs = model.fit_transform(df['Abstract'].tolist())

# Add the generated topics to the dataframe
df['Generated Topic'] = topics

df.columns

import random
institutions = [
    "Harvard University", "MIT", "Stanford University",
    "Oxford University", "Cambridge University",
    "National University of Singapore", "ETH Zurich",
    "University of Tokyo", "Imperial College London", "Caltech"
]

countries = [
    "USA", "UK", "Germany", "Japan", "Canada",
    "Australia", "India", "China", "Switzerland", "Singapore"
]

# Add random institutions and countries to the DataFrame
df['Institutions'] = [random.choice(institutions) for _ in range(len(df))]
df['Countries'] = [random.choice(countries) for _ in range(len(df))]



########topic affiliation N/w###########
import networkx as nx

# Create a graph
G = nx.Graph()

# Add edges for authors and topics
for topic, authors in authors_by_topic.items():
    for author in authors:
        G.add_edge(topic, author)

# Add institutions or countries if available in the metadata
if 'Institutions' in df.columns:
    for _, row in df.iterrows():
        topic = row['Topic name']
        institution = row['Institutions']
        G.add_edge(topic, institution)

# Visualize the network
print(f"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")

# Find top contributors (degree centrality)
centrality = nx.degree_centrality(G)
top_contributors = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]
print("Top contributors by centrality:")
for node, score in top_contributors:
    print(f"{node}: {score:.2f}")

# Export graph for visualization
nx.write_gexf(G, "topic_affiliation_network.gexf")

import matplotlib.pyplot as plt

# Draw the network
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=0.1)
nx.draw(G, pos, with_labels=True, node_size=50, font_size=8, edge_color="gray")
plt.title("Topic Affiliation Network")
plt.show()

import matplotlib.pyplot as plt
import networkx as nx

# Assuming G is your graph and pos is the layout for the nodes
pos = nx.spring_layout(G)  # Use any layout of your choice

# Draw the network
plt.figure(figsize=(8, 8))
nx.draw(G, pos, with_labels=False, node_size=50, edge_color="gray", font_size=8)

# Add labels only for nodes with degree > threshold
threshold = 10
labels = {node: node for node, degree in G.degree() if degree > threshold}
nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_color="red")

# Add title and show the plot
plt.title("Filtered Node Labels (Degree > 10)", fontsize=16)
plt.show()

df.head(5)



"""Keyword Co-occurrence Networks:
        Build a network graph where nodes are keywords, and edges represent co-occurrences in abstracts.
"""

df.columns

pip install keybert

"""It constructs a graph where each node represents a keyword, and an edge between two nodes represents their co-occurrence in the same abstract. The weight of each edge corresponds to the co-occurrence count.

This code extracts keywords from abstracts in a dataset using the KeyBERT model, then creates a network graph to visualize the co-occurrence of keywords. It first extracts the top 5 keywords (or keyphrases) from each abstract in the DataFrame df. Then, it counts the frequency of each keyword, filters out less frequent ones, and generates pairs of co-occurring keywords within each abstract. These co-occurrence pairs are counted, and a graph is built where nodes represent keywords, and edges represent co-occurrences with weights based on their frequency. The graph is visualized using a force-directed layout, with node sizes proportional to their degree and edge widths based on the co-occurrence weight. Labels are added to key nodes, and the graph is saved as a GEXF file for further analysis.
"""

import pandas as pd
import itertools
import networkx as nx
import matplotlib.pyplot as plt
from keybert import KeyBERT
from collections import Counter

# # Load your data (replace with your file path)
# file_path = 'path_to_your_csv.csv'
# df = pd.read_csv(file_path)

# Initialize KeyBERT model
kw_model = KeyBERT()

# Extract keywords from each abstract
def extract_keywords(text, top_n=5):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw[0] for kw in keywords]  # Only keep the keyword string

df['Keywords'] = df['Abstract'].apply(lambda x: extract_keywords(str(x), top_n=5))

# Flatten keywords for co-occurrence
all_keywords = list(itertools.chain.from_iterable(df['Keywords']))

# Count keyword frequencies
keyword_counts = Counter(all_keywords)

# Optionally filter keywords by frequency (keep keywords that occur more than a threshold)
min_frequency = 2
filtered_keywords = {k for k, v in keyword_counts.items() if v >= min_frequency}

# Create co-occurrence pairs
co_occurrence_pairs = []
for keywords in df['Keywords']:
    # Filter keywords in each abstract
    filtered = [kw for kw in keywords if kw in filtered_keywords]
    co_occurrence_pairs.extend(itertools.combinations(filtered, 2))

# Count co-occurrence frequencies
co_occurrence_counts = Counter(co_occurrence_pairs)

# Build the graph
G = nx.Graph()
for (kw1, kw2), weight in co_occurrence_counts.items():
    G.add_edge(kw1, kw2, weight=weight)

# Visualize the graph
plt.figure(figsize=(8, 8))
pos = nx.spring_layout(G, k=0.1)  # Force-directed layout

# Draw nodes with size based on their degree
node_sizes = [10 * nx.degree(G, node) for node in G.nodes()]
nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="lightblue")

# Draw edges with width based on weight
edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
nx.draw_networkx_edges(G, pos, width=[w / 10 for w in edge_weights], alpha=0.5)

# Add labels for key nodes
labels = {node: node for node in G.nodes() if nx.degree(G, node) > 5}
nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_color="red")

plt.title("Keyword Co-occurrence Network", fontsize=16)
plt.show()

# Save the graph as a file if needed
nx.write_gexf(G, "keyword_co_occurrence_network.gexf")

# Optionally filter keywords by frequency (keep keywords that occur more than a threshold)
min_frequency = 2
filtered_keywords = {k for k, v in keyword_counts.items() if v >= min_frequency}

# Create co-occurrence pairs
co_occurrence_pairs = []
for keywords in df['Keywords']:
    # Filter keywords in each abstract
    filtered = [kw for kw in keywords if kw in filtered_keywords]
    co_occurrence_pairs.extend(itertools.combinations(filtered, 2))

# Count co-occurrence frequencies
co_occurrence_counts = Counter(co_occurrence_pairs)

# Build the graph
G = nx.Graph()
for (kw1, kw2), weight in co_occurrence_counts.items():
    G.add_edge(kw1, kw2, weight=weight)

# Visualize the graph
plt.figure(figsize=(8, 8))
pos = nx.spring_layout(G, k=0.1)  # Force-directed layout

# Draw nodes with size based on their degree
node_sizes = [10 * nx.degree(G, node) for node in G.nodes()]
nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="lightblue")

# Draw edges with width based on weight
edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
nx.draw_networkx_edges(G, pos, width=[w / 10 for w in edge_weights], alpha=0.5)

# Add labels for key nodes
labels = {node: node for node in G.nodes() if nx.degree(G, node) > 5}
nx.draw_networkx_labels(G, pos, labels=labels, font_size=10, font_color="red")

plt.title("Keyword Co-occurrence Network", fontsize=16)
plt.show()





"""analysis from previous file

expand keywords.....
"""

df.columns

def expand_keywords(excel_file, keyword_column='Keywords'):
    """Expands a DataFrame by repeating rows for each keyword."""

    # df = pd.read_csv('/content/drive/MyDrive/Colab_data/Abstracts.csv') # Or engine='xlrd' if needed

    # 1. Handle various keyword separators (comma, semicolon, etc.)
    df[keyword_column] = df[keyword_column].astype(str).str.replace(';', ',').str.replace('and ', ',').str.split(',')

    expanded_rows = []
    for _, row in df.iterrows():
        keywords = row[keyword_column]
        if isinstance(keywords, list) and len(keywords) > 0 and keywords[0] != 'nan': # Check for valid keyword lists
            for keyword in keywords:
                new_row = row.copy()
                new_row[keyword_column] = keyword.strip()
                expanded_rows.append(new_row)
        elif isinstance(keywords, list) and len(keywords) > 0 and keywords[0] == 'nan':
            new_row = row.copy()
            new_row[keyword_column] = ""
            expanded_rows.append(new_row)
        else:
            new_row = row.copy()  # Keep rows with no keywords or errors
            expanded_rows.append(new_row)

    expanded_df = pd.DataFrame(expanded_rows)
    return expanded_df


# Example usage:
expanded_data = expand_keywords(df)  # Replace 'your_excel_file.xlsx' with your file name
print(expanded_data)
# expanded_data.to_csv('expanded_file.csv', index=False) #save to new excel file

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans  # Or another clustering algorithm
import sklearn.manifold as tsne
import matplotlib.pyplot as plt

abstracts = df['Abstract'].tolist()  # Assuming 'Abstract' is the column name

# 2. Preprocessing (important for meaningful results)
nltk.download('punkt_tab')  # For tokenization
nltk.download('stopwords')  # For stop word removal

stop_words = set(nltk.corpus.stopwords.words('english'))  # Customize stop words if needed

def preprocess_text(text):
    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase
    filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words]  # Remove punctuation and stop words
    return " ".join(filtered_tokens)

preprocessed_abstracts = [preprocess_text(abstract) for abstract in abstracts]

# 3. TF-IDF Vectorization
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_abstracts)

# 4. Cosine Similarity
similarity_matrix = cosine_similarity(tfidf_matrix)

# 5. Clustering (BEFORE expanding)
num_clusters = 5  # Choose an appropriate number
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Fit KMeans on the original similarity matrix (not the expanded one)
# clusters = kmeans.fit_predict(similarity_matrix) tfidf_matrix
clusters = kmeans.fit_predict(similarity_matrix)

# Add cluster labels to the ORIGINAL DataFrame
df['Cluster'] = clusters  # Now the lengths should match

# NOW expand the DataFrame based on keywords
expanded_df = expand_keywords(df.copy(), keyword_column='Keywords') # Pass a copy to avoid modifying the original df

# 6. Dimensionality Reduction (using t-SNE or UMAP - on the ORIGINAL similarity matrix)
from sklearn.manifold import TSNE  # Import t-SNE
reducer = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)  # Or UMAP
embedding = reducer.fit_transform(similarity_matrix) # Use the original similarity matrix


# 7. Visualization (use expanded_df for colors)
import matplotlib.pyplot as plt
plt.figure(figsize=(7, 7))
for cluster in range(num_clusters):
    plt.scatter(embedding[df['Cluster'] == cluster, 0], embedding[df['Cluster'] == cluster, 1], label=f"Cluster {cluster}")
plt.title("Abstract Similarity Clusters")
plt.legend()
plt.show()
plt.savefig("figure.png")  # Saves the current figure to the PDF colab_data_path +
plt.close()


# 8. Analyze Clusters (example - print top keywords per cluster)
for cluster in range(num_clusters):
    cluster_docs = df[df['Cluster'] == cluster]
    keywords = ",".join(cluster_docs['Keywords'].astype(str)).split(",")  # Combine and split keywords
    keyword_counts = pd.Series(keywords).value_counts()  # Count keyword frequencies
    print(f"\nTop Keywords for Cluster {cluster}:")
    print(keyword_counts.head(5))  # Print top 10 keywords

# import ast
# expanded_df['Keywords'] = expanded_df['Keywords'].apply(ast.literal_eval)


import ast

def clean_keywords(row):
    try:
        # Convert the string representation of the list to an actual list
        return ast.literal_eval(row)
    except (ValueError, SyntaxError):
        # Return an empty list if parsing fails
        return []

# Apply the cleaning function to the Keywords column
expanded_df['Keywords'] = expanded_df['Keywords'].apply(clean_keywords)

print(expanded_df['Keywords'].head(10))  # Check cleaned entries
print(type(expanded_df['Keywords'][0]))

#use pre trained emb for keywords

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

# Load pre-trained embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast

# Generate embeddings for keywords
keywords = expanded_df['Keywords'].unique()  # Unique keywords
keyword_embeddings = model.encode(keywords)

# Perform KMeans clustering to group keywords
num_clusters = 10  # Adjust as needed
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(keyword_embeddings)

# Map keywords to their cluster labels
keyword_cluster_map = {keyword: cluster for keyword, cluster in zip(keywords, kmeans.labels_)}

# Add a cluster label to each row in the DataFrame
expanded_df['cluster'] = expanded_df['Keywords'].map(keyword_cluster_map)

# Optionally replace clusters with a representative keyword
cluster_representatives = {cluster: keywords[kmeans.labels_ == cluster][0] for cluster in range(num_clusters)}
expanded_df['keyword1_cluster'] = expanded_df['cluster'].map(cluster_representatives)

import matplotlib.pyplot as plt

# Group by year and cluster
cluster_trends = expanded_df.groupby(['Year', 'keyword1_cluster']).size().unstack(fill_value=0)

# Plot trends for top clusters
top_clusters = cluster_trends.sum(axis=0).sort_values(ascending=False).head(10).index
cluster_trends[top_clusters].plot(figsize=(10, 6))
plt.title("Trends of Top Keyword Clusters Over Years")
plt.xlabel("Year")
plt.ylabel("Number of Papers")
plt.legend(title="Keyword Clusters")
plt.show()

#########################################################################################################pip install prophet

pip install prophet

df_trends = expanded_df.groupby(['Year', 'keyword1_cluster']).size().reset_index(name='count')
print(df_trends.head())

# Pivot the data for visualization
pivot_df = df_trends.pivot(index='Year', columns='keyword1_cluster', values='count').fillna(0)

# Plot the trends
pivot_df.plot(figsize=(12, 6), marker='o')
plt.title('Keyword Cluster Trends Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Papers')
plt.legend(title='Keyword Clusters')
plt.show()

pip install transformers

from transformers import AutoTokenizer, AutoModel
import torch

# Load pretrained model and tokenizer
model_name = "roberta-base"  # You can replace this with another model like "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Example: Embedding a keyword
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        # Use the CLS token embedding (first token) for the entire sequence
        embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)
    return embedding.numpy()

# Apply to keywords
expanded_df['keyword_embedding'] = expanded_df['keyword1_cluster'].apply(get_embedding)

from sklearn.cluster import KMeans
import numpy as np

# Stack all embeddings into a matrix
embeddings = np.vstack(expanded_df['keyword_embedding'].values)

# Cluster the embeddings
n_clusters = 4  # Adjust as needed
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
expanded_df['clusterss'] = kmeans.fit_predict(embeddings)

print(expanded_df[['keyword1_cluster', 'clusterss']].head())

# Group data by year and cluster
expanded_df = expanded_df.groupby(['Year', 'clusterss']).size().reset_index(name='count')

from prophet import Prophet

for cluster in expanded_df['clusterss'].unique():
    # Filter data for the specific cluster
    cluster_data = expanded_df[expanded_df['clusterss'] == cluster][['Year', 'count']].rename(columns={'Year': 'ds', 'count': 'y'})

    # Convert 'ds' to proper datetime format (start of the year)
    cluster_data['ds'] = pd.to_datetime(cluster_data['ds'], format='%Y')

    # Skip clusters with insufficient data
    if len(cluster_data) < 2:
        continue

    # Initialize and fit the Prophet model
    model = Prophet()
    model.fit(cluster_data)

    # Create future DataFrame
    future = model.make_future_dataframe(periods=5, freq='Y')
    future['ds'] = pd.to_datetime(future['ds'], format='%Y')  # Ensure datetime format for future dates

    # Forecast
    forecast = model.predict(future)

    # Plot
    model.plot(forecast)
    plt.title(f'Future Trend for Cluster {cluster}')
    plt.show()

# Get the unique keywords in cluster 1
unique_keywords_cluster_1 = expanded_df[expanded_df['clusterss'] == 0]['clusterss'].unique()

# Print the list of unique keywords
print(unique_keywords_cluster_1)
# Convert to list if you want a Python list
unique_keywords_cluster_1_list = unique_keywords_cluster_1.tolist()

# Print the list of unique keywords
print(unique_keywords_cluster_1_list)

expanded_df.columns

