# -*- coding: utf-8 -*-
"""Abstractanalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z59obLjmkxa5cpxTIhy_I2byltg0S_ji

python3 -m venv ~/myenv
source ~/myenv/bin/activate
pip install pandas


pip install keybert pandas

pip install scikit-learn sentence-transformers numpy
"""
import pandas as pd


# Load data from CSV
def load_data(file_path):
    return pd.read_csv(file_path)

# Extract keywords using KeyBERT
def extract_keywords_keybert(abstract, model):
    try:
        keywords = model.extract_keywords(abstract, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
        return "; ".join([kw[0] for kw in keywords])  # Join top keywords as a single string
    except Exception as e:
        print(f"Error extracting keywords: {e}")
        return ""

# Process all abstracts
def process_data(data, model):
    data['extracted_keywords'] = data['Abstract'].apply(lambda x: extract_keywords_keybert(x, model))
    return data

# Save the results to a CSV file
def save_results(data, output_file):
    data.to_csv(output_file, index=False)
    print(f"Results saved to {output_file}")

#from google.colab import drive
#drive.mount('/content/drive/')

#from google.colab import files
from keybert import KeyBERT

# Upload the file
#uploaded = files.upload()

# Main Execution
if __name__ == "__main__":
    input_file = "AbstractsCleanedFinal.csv"  # Replace with your input CSV file

    output_file = "abstracts_with_keywordsUpdatedlast9000.csv"
    header = pd.read_csv (input_file,nrows=0).columns
    data = pd.read_csv(input_file).head(5)
    data=pd.read_csv(input_file,skiprows=25000,nrows=9000,names=header,header=None)
    print (data.shape)
    #data = pd.read_csv(input_file)
    # Load the dataset
    #data = load_data(input_file)

    # Initialize KeyBERT model
    kw_model = KeyBERT()

    # Extract keywords for each abstract
    data_with_keywords = process_data(data, kw_model)

    # Save results
    save_results(data_with_keywords, output_file)

#!ls
"""
files.download(output_file)

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import networkx as nx

from google.colab import files
import pandas as pd

# Upload the file
uploaded = files.upload()

# Load the file into a DataFrame
file_name = list(uploaded.keys())[0]  # Get the uploaded file name
data = pd.read_csv(file_name)

print(f"Loaded {len(data)} rows from {file_name}")

# Load the data
#data = pd.read_csv("abstracts_with_keywords.csv")

# Ensure columns exist (Adjust column names if needed)
if 'extracted_keywords' not in data.columns or 'topics' not in data.columns:
    raise ValueError("The dataset must contain 'extracted_keywords' and 'topics' columns.")

# Split keywords and topics into lists
data['keywords'] = data['extracted_keywords'].apply(lambda x: x.split("; "))
data['topics'] = data['topics'].apply(lambda x: x.split("; "))



# 1. Keyword Frequency Analysis
def plot_keyword_frequency(data):
    all_keywords = [kw for keywords in data['keywords'] for kw in keywords]
    keyword_counts = pd.Series(all_keywords).value_counts().head(20)

    # Plot bar chart
    plt.figure(figsize=(10, 6))
    keyword_counts.sort_values(ascending=True).plot(kind='barh', color='skyblue')
    plt.title("Top 20 Keywords")
    plt.xlabel("Frequency")
    plt.ylabel("Keywords")
    plt.show()

# 2. Topic Trends Over Time
def plot_topic_trends(data):
    # Ensure 'year' is in numeric format
    data['year'] = pd.to_datetime(data['year'], errors='coerce').dt.year
    data = data[data['year'].between(2014, 2024)]
    # Explode topics and group by year
    topic_trends = data.explode('topics').groupby(['year', 'topics']).size().unstack(fill_value=0)

    # Filter to show top 10 topics (by total count across years)
    top_topics = topic_trends.sum().sort_values(ascending=False).head(10).index
    topic_trends = topic_trends[top_topics]

    # Plot trends
    plt.figure(figsize=(14, 8))  # Increased figure size
    for topic in topic_trends.columns:
        plt.plot(topic_trends.index, topic_trends[topic], label=topic)

    plt.title("Top 10 Topic Trends Over Time", fontsize=16)
    plt.xlabel("Year", fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.legend(title="Topics", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)  # Legend outside the plot
    plt.grid(alpha=0.5)
    plt.tight_layout()  # Adjust layout
    plt.show()

import matplotlib.pyplot as plt


# Step 1: Ensure 'year' column is numeric and filter valid years
data['year'] = pd.to_numeric(data['year'], errors='coerce')  # Convert to numeric, NaN for invalid
data = data[data['year'].between(2014, 2024)]  # Filter years within the range


data['topics'] = data['topics'].astype(str).str.split(';')
data = data.explode('topics')
data['topics'] = data['topics'].str.strip()  # Remove extra spaces


# Step 2: Group by year and keyword
keyword_trends = data.groupby(['year', 'topics']).size().unstack(fill_value=0)



print(data.head())  # Check the first few rows of the dataset
print(data['topics'].unique())  # Ensure keywords are present
print(keyword_trends.head())  # Check the keyword trends dataframe

# Step 3: Select top 20 keywords by total frequency
top_keywords = keyword_trends.sum().sort_values(ascending=False).head(5).index
keyword_trends = keyword_trends[top_keywords]





# Step 4: Plot the trends
plt.figure(figsize=(14, 8))
for keyword in keyword_trends.columns:
    plt.plot(keyword_trends.index, keyword_trends[keyword], label=keyword)

plt.title("Trend of Top 5 topics Over Time", fontsize=16)
plt.xlabel("Year", fontsize=12)
plt.ylabel("Frequency", fontsize=12)
plt.legend(title="Topics", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
plt.grid(alpha=0.5)
plt.ylim(1, 10)
plt.tight_layout()
plt.show()







# Run analyses and visualizations
plot_keyword_frequency(data)
#plot_topic_trends(data)
#plot_co_occurrence_network(data)
#plot_heatmap(data)
"""
